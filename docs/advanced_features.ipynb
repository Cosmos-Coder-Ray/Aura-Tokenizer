{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4cc42ab",
   "metadata": {},
   "source": [
    "# AuraTokenizer: Advanced Features and Integrations\n",
    "\n",
    "This notebook demonstrates advanced features, integrations, and development workflows for the AuraTokenizer project. We'll cover:\n",
    "\n",
    "1. Development Environment Setup\n",
    "2. Advanced Tokenization Features\n",
    "3. Cross-Language Integration\n",
    "4. Performance Benchmarking\n",
    "5. Custom Extensions\n",
    "6. Advanced Testing\n",
    "7. Documentation Generation\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac021ab1",
   "metadata": {},
   "source": [
    "## 1. Development Environment Setup\n",
    "\n",
    "First, let's set up our development environment and import the necessary modules. We'll install AuraTokenizer and its dependencies:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339cfbf2",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "To build and use AuraTokenizer, you'll need:\n",
    "\n",
    "1. Python 3.7 or later\n",
    "2. Rust toolchain (can be installed from https://rustup.rs/)\n",
    "3. A C++ compiler (MSVC on Windows, GCC on Linux/macOS)\n",
    "4. CMake\n",
    "\n",
    "Please refer to the project's README for detailed installation instructions. The package needs to be built from source as it contains Rust and C++ components.\n",
    "\n",
    "For this notebook, we'll assume you have already:\n",
    "1. Clone the repository\n",
    "2. Install the prerequisites\n",
    "3. Build and install the package using:\n",
    "   ```bash\n",
    "   python -m pip install maturin\n",
    "   maturin develop\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f78c2",
   "metadata": {},
   "source": [
    "### Building from Source\n",
    "\n",
    "The AuraTokenizer package consists of Python, Rust, and C++ components. Here's the complete build process:\n",
    "\n",
    "1. Build the C++ components:\n",
    "   ```bash\n",
    "   # Windows (PowerShell)\n",
    "   cmake -S . -B build\n",
    "   cmake --build build --config Release\n",
    "\n",
    "   # Linux/macOS\n",
    "   cmake -S . -B build\n",
    "   cmake --build build\n",
    "   ```\n",
    "\n",
    "2. Build the Rust components:\n",
    "   ```bash\n",
    "   cd aura_tokenizer_bridge\n",
    "   cargo build --release\n",
    "   ```\n",
    "\n",
    "3. Build and install the Python package:\n",
    "   ```bash\n",
    "   python -m pip install maturin\n",
    "   maturin develop\n",
    "   ```\n",
    "\n",
    "If you encounter any issues:\n",
    "- Make sure all prerequisites are installed\n",
    "- Check that environment variables (PATH, RUSTUP_HOME, etc.) are properly set\n",
    "- Consult the troubleshooting section in the project documentation\n",
    "\n",
    "For this notebook to work, you must complete the build process first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f41379",
   "metadata": {},
   "source": [
    "### Common Build Issues\n",
    "\n",
    "#### CMake Errors\n",
    "\n",
    "If you encounter a CMake parsing error like:\n",
    "```\n",
    "CMake Error at CMakeLists.txt:192:\n",
    "Parse error. Expected a command name, got right paren with text \")\".\n",
    "```\n",
    "\n",
    "This error is caused by an extra closing parenthesis in the CMakeLists.txt file. To fix it:\n",
    "\n",
    "1. Open `Aura-Tokenizer/CMakeLists.txt`\n",
    "2. Look for this section (around line 187-192):\n",
    "   ```cmake\n",
    "   if(WIN32)\n",
    "       set_target_properties(auratokenizer PROPERTIES\n",
    "           WINDOWS_EXPORT_ALL_SYMBOLS ON\n",
    "       )\n",
    "   endif()\n",
    "   )  # <- Remove this extra closing parenthesis\n",
    "   ```\n",
    "3. Remove the extra closing parenthesis after `endif()`\n",
    "4. Save the file and try building again:\n",
    "   ```bash\n",
    "   cmake -S . -B build\n",
    "   cmake --build build --config Release\n",
    "   ```\n",
    "\n",
    "If you encounter other CMake issues:\n",
    "1. Make sure you're in the correct directory:\n",
    "   ```bash\n",
    "   # Navigate to the Aura-Tokenizer subdirectory\n",
    "   cd Aura-Tokenizer\n",
    "   ```\n",
    "\n",
    "2. Try cleaning the build directory:\n",
    "   ```bash\n",
    "   # Windows\n",
    "   rmdir /s /q build\n",
    "   # Linux/macOS\n",
    "   rm -rf build\n",
    "   ```\n",
    "\n",
    "3. Run CMake with verbose output to see more details:\n",
    "   ```bash\n",
    "   cmake -S . -B build --log-level=VERBOSE\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca044a6",
   "metadata": {},
   "source": [
    "#### Include Directory Issues\n",
    "\n",
    "If you see CMake errors about INTERFACE_INCLUDE_DIRECTORIES:\n",
    "```\n",
    "Target \"auratokenizer\" INTERFACE_INCLUDE_DIRECTORIES property contains path:\n",
    "which is prefixed in the source directory.\n",
    "```\n",
    "\n",
    "This happens when include directories are not properly configured for installation. The fix involves:\n",
    "\n",
    "1. Using generator expressions for public includes:\n",
    "   ```cmake\n",
    "   target_include_directories(auratokenizer\n",
    "       PUBLIC\n",
    "           $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\n",
    "           $<INSTALL_INTERFACE:include>\n",
    "       PRIVATE\n",
    "           ${OTHER_INCLUDE_DIRS}\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. Making dependency includes PRIVATE unless they're part of the public interface\n",
    "\n",
    "The build should now proceed without include directory errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dbee8d",
   "metadata": {},
   "source": [
    "#### GTest Dependency\n",
    "\n",
    "If you encounter a GTest dependency error:\n",
    "```\n",
    "Could NOT find GTest (missing: GTEST_LIBRARY GTEST_INCLUDE_DIR GTEST_MAIN_LIBRARY)\n",
    "```\n",
    "\n",
    "You have two options:\n",
    "\n",
    "1. Skip building tests (recommended for initial setup):\n",
    "   ```bash\n",
    "   cmake -S . -B build -DBUILD_TESTS=OFF\n",
    "   ```\n",
    "\n",
    "2. Install GTest if you want to build tests:\n",
    "   ```bash\n",
    "   # Windows with vcpkg\n",
    "   vcpkg install gtest:x64-windows\n",
    "   \n",
    "   # Linux\n",
    "   sudo apt-get install libgtest-dev\n",
    "   \n",
    "   # Then configure with tests enabled\n",
    "   cmake -S . -B build -DBUILD_TESTS=ON\n",
    "   ```\n",
    "\n",
    "The core library will build without GTest. Testing is optional and can be enabled later when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4be67192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: c:\\Users\\rstiw\\Desktop\\AURA\\python\\python.exe -m pip install maturin\n",
      "\n",
      "Building and installing AuraTokenizer...\n",
      "Running: c:\\Users\\rstiw\\Desktop\\AURA\\python\\python.exe -m pip install -e . --no-deps\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['c:\\\\Users\\\\rstiw\\\\Desktop\\\\AURA\\\\python\\\\python.exe', '-m', 'pip', 'install', '-e', '.', '--no-deps']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Build and install the package\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBuilding and installing AuraTokenizer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mrun_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-m\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstall\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-e\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m--no-deps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mrun_command\u001b[39m\u001b[34m(cmd)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_command\u001b[39m(cmd):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(cmd)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rstiw\\Desktop\\AURA\\python\\Lib\\subprocess.py:571\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     retcode = process.poll()\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    572\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['c:\\\\Users\\\\rstiw\\\\Desktop\\\\AURA\\\\python\\\\python.exe', '-m', 'pip', 'install', '-e', '.', '--no-deps']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def run_command(cmd):\n",
    "    print(f\"Running: {' '.join(cmd)}\")\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "# Install build dependencies\n",
    "run_command([sys.executable, \"-m\", \"pip\", \"install\", \"maturin\"])\n",
    "\n",
    "# Build and install the package\n",
    "print(\"\\nBuilding and installing AuraTokenizer...\")\n",
    "run_command([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \".\", \"--no-deps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d01cdeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.10 (tags/v3.12.10:0cc8128, Apr  8 2025, 12:21:36) [MSC v.1943 64 bit (AMD64)]\n",
      "Current directory: c:\\Users\\rstiw\\Desktop\\AURA\\Packages\\Aura-Tokenizer\\docs\\examples\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f94a6a",
   "metadata": {},
   "source": [
    "## 2. Basic Tokenizer Setup\n",
    "\n",
    "Once AuraTokenizer is installed, we can create a simple tokenizer with basic configuration. This section demonstrates the core functionality before we move on to advanced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dba88cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AuraTokenizer is not installed or cannot be imported.\n",
      "Error: cannot import name 'PyBPETokenizer' from 'aura_tokenizer.core' (c:\\Users\\rstiw\\Desktop\\AURA\\python\\Lib\\site-packages\\aura_tokenizer\\core.cp312-win_amd64.pyd)\n",
      "\n",
      "Please follow the installation instructions in the prerequisites section.\n"
     ]
    }
   ],
   "source": [
    "# Try importing aura_tokenizer if available\n",
    "try:\n",
    "    from aura_tokenizer import AuraTokenizer\n",
    "    print(\"AuraTokenizer is installed and imported successfully.\")\n",
    "except ImportError as e:\n",
    "    print(\"AuraTokenizer is not installed or cannot be imported.\")\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(\"\\nPlease follow the installation instructions in the prerequisites section.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e9036cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not create tokenizer: cannot import name 'PyBPETokenizer' from 'aura_tokenizer.core' (c:\\Users\\rstiw\\Desktop\\AURA\\python\\Lib\\site-packages\\aura_tokenizer\\core.cp312-win_amd64.pyd)\n"
     ]
    }
   ],
   "source": [
    "def create_basic_tokenizer():\n",
    "    \"\"\"Create a basic character-level tokenizer for demonstration.\"\"\"\n",
    "    try:\n",
    "        from aura_tokenizer import AuraTokenizer, TokenizerConfig\n",
    "        \n",
    "        config = TokenizerConfig()\n",
    "        config.type = \"char\"  # Character-level tokenization\n",
    "        config.add_special_tokens = True\n",
    "        \n",
    "        tokenizer = AuraTokenizer(config)\n",
    "        return tokenizer\n",
    "    except ImportError as e:\n",
    "        print(f\"Could not create tokenizer: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Try creating and using a basic tokenizer\n",
    "tokenizer = create_basic_tokenizer()\n",
    "if tokenizer is not None:\n",
    "    text = \"Hello, World!\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    print(f\"\\nInput text: {text}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    print(f\"Decoded text: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6496098",
   "metadata": {},
   "source": [
    "### Current Status\n",
    "\n",
    "As shown above, the package needs to be properly built and installed before we can proceed with the examples. The error message indicates that some core components are missing or not properly built.\n",
    "\n",
    "**Next Steps:**\n",
    "1. Exit this notebook\n",
    "2. Follow the build instructions in the \"Building from Source\" section above\n",
    "3. Verify the installation by running:\n",
    "   ```python\n",
    "   python -c \"import aura_tokenizer; print(aura_tokenizer.__file__)\"\n",
    "   ```\n",
    "4. Return to this notebook once the installation is successful\n",
    "\n",
    "The remaining sections of this notebook will demonstrate advanced features once you have a working installation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4325e054",
   "metadata": {},
   "source": [
    "## 3. Advanced Tokenization Features\n",
    "\n",
    "*This section will cover:*\n",
    "- Custom tokenization models\n",
    "- Pre-tokenizers and post-processors\n",
    "- Training custom vocabularies\n",
    "- Handling special tokens\n",
    "- Unicode normalization\n",
    "\n",
    "## 4. Cross-Language Integration\n",
    "\n",
    "*This section will demonstrate:*\n",
    "- Using Rust components\n",
    "- C++ integration\n",
    "- FFI interfaces\n",
    "- Performance optimizations\n",
    "\n",
    "## 5. Performance Benchmarking\n",
    "\n",
    "*This section will cover:*\n",
    "- Throughput measurements\n",
    "- Memory usage analysis\n",
    "- Comparison with other tokenizers\n",
    "- Optimization techniques\n",
    "\n",
    "## 6. Custom Extensions\n",
    "\n",
    "*This section will show:*\n",
    "- Creating custom pre-tokenizers\n",
    "- Implementing custom post-processors\n",
    "- Adding new tokenization algorithms\n",
    "- Plugin development\n",
    "\n",
    "## 7. Advanced Testing\n",
    "\n",
    "*This section will demonstrate:*\n",
    "- Unit test creation\n",
    "- Integration testing\n",
    "- Performance testing\n",
    "- Cross-language testing\n",
    "\n",
    "## 8. Documentation Generation\n",
    "\n",
    "*This section will cover:*\n",
    "- API documentation\n",
    "- Example generation\n",
    "- Documentation testing\n",
    "- Cross-language documentation\n",
    "\n",
    "Please complete the installation steps before proceeding with these sections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
